install.packages(c("kernlab", "mlbench", "ggplot2"))
library(kernlab)
library(mlbench)
library(ggplot2)
set.seed(5546)
dataTN <- mlbench.threenorm(100, 2)
dataTN <- data.frame(class=dataTN$classes, dataTN$x)
testdata <- dataTN[1:30,]
Loss <- function(w, y, x ){
levels(y) <- c(-1,1)
y <- as.numeric(y)
L <- c()
for(i in 1:nrow(x)){
m <-  y[i]*(sum(w[2:3]*x[i,])+w[1])
L[i] <- max(0, 1-m)
}
sum(L)
}
marginOpt <- function(w, y, x, C=1){
0.5*sum(w^2)+C*Loss(w,y,x)
}
optim(par=c(1,1,1), fn=marginOpt, y=testdata[,1], x=testdata[,2:3])
optim(par=c(1,1,1), fn=marginOpt, y=dataTN[,1], x=dataTN[,2:3])
marginOpt(c(1,1,1),y=dataTN[,1], x=dataTN[,2:3] )
marginOpt(c(.1,.1,.1),y=dataTN[,1], x=dataTN[,2:3] )
m <- ksvm(class~., data=dataTN[1:30,], kernel="vanilladot", scale=FALSE)
m
coef(m)[[1]] %*% as.matrix(testdata[SVindex(m) ,2:3])
m <- ksvm(class~., data=dataTN[1:30,], kernel="vanilladot", scaled=FALSE)
coef(m)[[1]] %*% as.matrix(testdata[SVindex(m) ,2:3])
HingeLoss <- function(w, y, x ){
levels(y) <- c(-1,1)
y <- as.numeric(y)
loss <- y*(w[2:3]%*%x + w[1])
loss
}
HingeLoss(c(1,1,1), testdata[,2:3], testdata[,1])
testdata
debug(HingeLoss)
testdata
HingeLoss(c(1,1,1), testdata[,2:3], testdata[,1])
HingeLoss(w=c(1,1,1), y=testdata[,2:3], x=testdata[,1])
undebug(HingeLoss)
HingeLoss(w=c(1,1,1), y=testdata[,2:3], x=testdata[,1])
HingeLoss(w=c(1,1,1), x=testdata[,2:3], y=testdata[,1])
HingeLoss <- function(w, y, x ){
levels(y) <- c(-1,1)
y <- as.numeric(y)
x <- as.matrix(x)
loss <- y*(w[2:3]%*%x + w[1])
loss
}
HingeLoss(w=c(1,1,1), x=testdata[,2:3], y=testdata[,1])
library(ggplot2)
library(rpart)
generateDiagData <- function(n=100, ...){
x1 <- sample.int(n,n, replace=T)
x2 <- sample.int(n,n, replace=T)
lab <- x1<x2
x1 <- jitter(x1, ...)
x2 <- jitter(x2, ...)
data.frame(x1,x2,lab)
}
data <- generateDiagData(150, factor=100)
ggplot(data) + geom_point(aes(x=x1, y=x2, color=lab))
tree <- rpart(lab~., data=data)
tree
tree$splits
rect=list(xmin=min(data$x1), xmax=max(data$x1), ymin=min(data$x2), ymax=max(data$x2))
rect
rect=data.frame(xmin=min(data$x1), xmax=max(data$x1), ymin=min(data$x2), ymax=max(data$x2))
rect
tree
str(tree)
tree$frame
tree$splits
colnames(tree$frame)
rownmaes(tree$frame)
rownames(tree$frame)
as.number(rownames(tree$frame))
as.integer(rownames(tree$frame))
order <- as.integer(rownames(tree$frame))
order
tree
tree$splits[order,]
xtfrm(order)
ÃŸo
setwd("~/Uni/Fprog/Hausuebung2")
library(devtools)
library(roxygen2)
library(testthat)
load_all("logitreg6/")
url <- "http://www.statistik.lmu.de/~scheipl/downloads/fortprog/"
source(paste0(url, "simulate_logitreg.R"))
data <- simulate_logitreg(seed = 555, q_numeric = 1, q_factor = 1, n = 100,
data.frame = TRUE)
m_0 <- logitreg(design = data[,-1], response = data[,1])
str(m_0)
m_0 <- logitreg(y ~ . , data = data)
m_0$coefficients
expect_equal(predict(m_0),
unname(predict(glm(y ~ ., data = data, family = binomial),
type = "response")),
tolerance = 0.0009)
predict(m_0)
expect_equal(predict(m_0),
predict(glm(y ~ ., data = data, family = binomial),
type = "response"),
tolerance = 0.0009)
new_data <- data.frame(X1 = rnorm(100),
X2 = sample(c(1,0), 100, replace = TRUE))
expect_equal(predict(m_0, new_data),
predict(glm(data$y ~ data$x[,-1], family = binomial),
newdata = new_data, type = "response"),
tolerance = 0.0009)
expect_equal(predict(m_0, new_data),
predict(glm(y ~ ., data = data, family = binomial),
newdata = new_data, type = "response"),
tolerance = 0.0009)
model_logitreg <- logitreg(y ~ . , data = data)
model_glm <- glm(y ~ ., data = data, family = binomial)
expect_equal(predict(model_logitreg),
predict(model_glm, type = "response"),
tolerance = 0.0009)
new_data <- data.frame(X1 = rnorm(100),
X2 = sample(c(1,0), 100, replace = TRUE))
expect_equal(predict(model_logitreg, new_data),
predict(model_glm, newdata = new_data, type = "response"),
tolerance = 0.0009)
expect_equal(fitted(model_logitreg), fitted(model_glm), tolerance = 0.0009)
plot(model_logitreg)
